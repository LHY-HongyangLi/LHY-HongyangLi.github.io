<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Hongyang Li, ÊùéÂºòÊ¥ã, Homepage, SCUT, South China University of Technology, ÂçéÂçóÁêÜÂ∑•Â§ßÂ≠¶, IDEA, International Digital Economy Academy, Á≤§Ê∏ØÊæ≥Â§ßÊπæÂå∫Êï∞Â≠óÁªèÊµéÁ†îÁ©∂Èô¢, ËÆ°ÁÆóÊú∫ËßÜËßâ‰∏éÊú∫Âô®‰∫∫, Computer Vision, ËÆ°ÁÆóÊú∫ËßÜËßâ, Deep Learning, Ê∑±Â∫¶Â≠¶‰π†, Machine Learning, Êú∫Âô®Â≠¶‰π†">
    <meta name="author" content="Hongyang Li, ÊùéÂºòÊ¥ã">
    <meta name="keywords" content="Hongyang Li, ÊùéÂºòÊ¥ã, Homepage, SCUT, South China University of Technology, ÂçéÂçóÁêÜÂ∑•Â§ßÂ≠¶, IDEA, International Digital Economy Academy, Á≤§Ê∏ØÊæ≥Â§ßÊπæÂå∫Êï∞Â≠óÁªèÊµéÁ†îÁ©∂Èô¢, ËÆ°ÁÆóÊú∫ËßÜËßâ‰∏éÊú∫Âô®‰∫∫, Computer Vision, ËÆ°ÁÆóÊú∫ËßÜËßâ, Deep Learning, Ê∑±Â∫¶Â≠¶‰π†, Machine Learning, Êú∫Âô®Â≠¶‰π†">
    <link rel="shortcut icon" href="/assets/ico/icon.ico">

    <title>About Me - Hongyang Li</title>

    <!-- Bootstrap core CSS -->
    <link href="/assets/css/bootstrap.min.css" rel="stylesheet">


    <!-- Custom styles for this template -->
    <link href="/assets/css/main.css" rel="stylesheet">
  </head>

  <body>
    <div class="container">
      <div class="header row">
        <div class="row">
    <div class="title pull-left">
      Hongyang Li
    </div>
    <ul class="nav nav-pills pull-right">
      
      
      <li class="active">
        <a href="/" title="">Home</a>
      </li>
      
      
      <li >
        <a href="/publications/" title="">Publications</a>
      </li>
      
    </ul>
</div>

<script>
  $()
</script>
       
      </div>
      <div class="content row">
        <div>
  <div class="img">
    <img alt="photo" src="/assets/images/hongyangli.png">
  </div>
  <div class="text">
    
    <h1 id="hongyang-li"><strong>Hongyang Li</strong></h1>

<p>Ph.D Student @ South China University of Technology<br />
Research Intern @ International Digital Economy Academy (IDEA)<br />
Guangdong-HongKong-Macau Greater Bay Area<br />
Shenzhen, China</p>

<p>Email: ftwangyeunglei AT mail dot scut dot edu dot cn</p>

<p><a href="https://scholar.google.com.hk/citations?user=zdgHNmkAAAAJ&hl=en">Google Scholar</a> | <a href="https://www.linkedin.com/in/hongyang-li-441856300">LinkedIn</a> | <a href="https://github.com/LHY-HongyangLi">Github</a></p>

  </div>
</div>
<div>
  
  <p>Hi! This is Hongyang Li, ÊùéÂºòÊ¥ã in Chinese. I‚Äôm a second-year year Ph.D. student (2022-now) at the <a href="http://www2.scut.edu.cn/ft/">Department of Future Technology</a>, <a href="http://www2.scut.edu.cn/">South China University of Technology</a>, supervised by Prof.<a href="https://www.leizhang.org">Lei Zhang</a>. I interned at <a href="https://www.idea.edu.cn/research/cvr.html">International Digital Economy Academy</a>. Previously, I obtained my bachelor‚Äôs degree from School of Electrical Engineering in South China University of Technology in 2021.</p> 

<p>üîñ My research interests lie in Tracking Any Point, 3D Perception and Multi-modal Models.</p>

<p>üí¨ Feel free to contact me for any discussion and coorperation.</p>


  <hr>
  <h1>
    News
  </h1>
  
<ul></ul>
  <li>
    <p>[2024/9] TAPTRv3 is released! Check out our <a href="https://arxiv.org/abs/2411.18671">TAPTRv3</a> for more details.</p>
  </li>
  <li>
    <p>[2024/9] TAPTRv2 is accepted by NeurIPS2024.</p>
  </li>
  <li>
    <p>[2024/7] TAPTRv2 is released! Check out our <a href="https://arxiv.org/abs/2407.16291">TAPTRv2</a> for more details.</p>
  </li>
  <li>
    <p>[2024/7] Two papers are accepted by ECCV2024! Check out our <a href="https://arxiv.org/abs/2403.13042">TAPTR</a> and <a href="https://arxiv.org/abs/2312.02949">LLaVA-Grounding</a> for more details.</p>
  </li>
  <li>
    <p>[2024/3] We release <a href="https://arxiv.org/abs/2403.13042">TAPTR</a>. Check out <a href="https://taptr.github.io">project page</a> for more details and online demos.</p>
  </li>
  <li>
    <p>[2023/12] We release <a href="https://arxiv.org/abs/2312.02949">LLaVA-Grounding</a>. <a href="https://llava-grounding.deepdataspace.com">Demo</a> and <a href="https://github.com/UX-Decoder/LLaVA-Grounding">inference code</a> are available.</p>
  </li>
  <li>
    <p>[2023/7] Two papers are accepted by ICCV2023. Check out our <a href="https://arxiv.org/abs/2302.13002">DFA3D</a> and <a href="https://arxiv.org/abs/2304.04742">StableDINO</a>.</p>
  </li>
  <li>
    <p>[2023/2] We release DA-BEV that establishes a new SOTA performance on nuScenes 3D detection leaderboard‚Äìcamera track. Check out our <a href="https://arxiv.org/abs/2302.13002">DA-BEV</a>.</p>
  </li>
  <li>
    <p>[2022/7] A paper is accepted by ECCV2022! Check out our <a href="https://arxiv.org/abs/2210.05232">DCL-Net</a>.</p>
  </li>
  <li>
    <p>[2021/9] A paper is accepted by NeurIPS2021! Check out our <a href="https://arxiv.org/abs/2111.07383">Sparse Steerable Convolution</a>.</p>
  </li>
</ul>


  <hr>
  <h1>
    Selected Publications
  </h1>
  
  <ol class="bibliography">
      <li>
        <div class="pub row">
      <div class="pub-image col-md-3">
        
        <img src="/assets/pdf/qu2024taptrv3.png" >
        
      </div>
      <div class="col-md-9">
        <span id="li2024taptrv3"><div class="csl-block csl-content"><div class="csl-block csl-title"><b>TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video</b></div><div class="csl-block csl-author"> Jinyuan Qu*, <b>Hongyang Li*</b>, Shilong Liu, Tianhe Ren, Zhaoyang Zeng, Lei Zhang</div><div class="csl-block csl-event"><i>ArXiv</i>, 2024</div></div></span>
    
        <span id="li2024taptrv3_materials">
          <ul class="tag">
            
        
            <span><a class="bib-materials" data-target="#li2024taptrv3_bibtex" data-toggle="collapse" href="#li2024taptrv3" onclick="return false" aria-expanded="false">BibTex</a></span>
        
        
            
              
              <span>
                <a class="bib-materials" href="https://arxiv.org/abs/2411.18671">arXiv</a>
                (<a class="bib-materials" href="https://arxiv.org/pdf/2411.18671">pdf</a>)
              </span>
              <span>
                <a class="project-page" href="https://taptr.github.io">Project Page</a>
              </span>
              <span>
                <a class="github" href="xxx">Github (Comming Soon)</a>
              </span>
            
    
    
            
    
          </ul>
        
          
        
          <pre id="li2024taptrv3_bibtex" class="pre pre-scrollable collapse">@article{Qu2024taptrv3,
            title={{TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video}},
            author={Qu, Jinyuan and Li, Hongyang and Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Zhang, Lei},
            journal={arXiv preprint},
            year={2024}
          }
    </pre>
        
        </span>
      </div>
    </div>
    </li>

    <li>
    <li>
      <div class="pub row">
    <div class="pub-image col-md-3">
      
      <img src="/assets/pdf/li2024taptrv2.png" >
      
    </div>
    <div class="col-md-9">
      <span id="li2024taptrv2"><div class="csl-block csl-content"><div class="csl-block csl-title"><b>TAPTRv2: Attention-based Position Update Improves Tracking Any Point</b></div><div class="csl-block csl-author"> <b>Hongyang Li</b>, Hao Zhang, Shilong Liu, Zhaoyang Zeng, Feng Li, Tianhe Ren, Bohan Li, Lei Zhang</div><div class="csl-block csl-event"><i>NeurIPS</i>, 2024</div></div></span>
  
      <span id="li2024taptrv2_materials">
        <ul class="tag">
          
      
          <span><a class="bib-materials" data-target="#li2024taptrv2_bibtex" data-toggle="collapse" href="#li2024taptrv2" onclick="return false" aria-expanded="false">BibTex</a></span>
      
      
          
            
            <span>
              <a class="bib-materials" href="https://arxiv.org/abs/2407.16291">arXiv</a>
              (<a class="bib-materials" href="https://arxiv.org/pdf/2407.16291">pdf</a>)
            </span>
            <span>
              <a class="project-page" href="https://taptr.github.io">Project Page</a>
            </span>
            <span>
              <a class="github" href="https://github.com/IDEA-Research/TAPTR">Github</a>
            </span>
          
  
  
          
  
        </ul>
      
        
      
        <pre id="li2024taptrv2_bibtex" class="pre pre-scrollable collapse">@article{li2024taptrv2,
          title={TAPTRv2: Attention-based Position Update Improves Tracking Any Point},
          author={Li, Hongyang and Zhang, Hao and Liu, Shilong and Zeng, Zhaoyang and Li, Feng and Ren, Tianhe and Bohan Li and Zhang, Lei},
          journal={arXiv preprint arXiv:2407.16291},
          year={2024}
        }
  </pre>
      
      </span>
    </div>
  </div>
  </li>

  <li>
    <div class="pub row">
  <div class="pub-image col-md-3">
    
    <img src="/assets/pdf/li2024taptr.png" >
    
  </div>
  <div class="col-md-9">
    <span id="li2024taptr"><div class="csl-block csl-content"><div class="csl-block csl-title"><b>TAPTR: Tracking Any Point with Transformers as Detection</b></div><div class="csl-block csl-author"> <b>Hongyang Li</b>, Hao Zhang, Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Lei Zhang</div><div class="csl-block csl-event"><i>ECCV</i>, 2024</div></div></span>

    <span id="li2024taptr_materials">
      <ul class="tag">
        
    
        <span><a class="bib-materials" data-target="#li2024taptr_bibtex" data-toggle="collapse" href="#li2024taptr" onclick="return false" aria-expanded="false">BibTex</a></span>
    
    
        
          
          <span>
            <a class="bib-materials" href="https://arxiv.org/abs/2403.13042">arXiv</a>
            (<a class="bib-materials" href="https://arxiv.org/pdf/2403.13042">pdf</a>)
          </span>
          <span>
            <a class="project-page" href="https://taptr.github.io">Project Page</a>
          </span>
          <span>
            <a class="github" href="https://github.com/IDEA-Research/TAPTR">Github</a>
          </span>
        


        

      </ul>
    
      
    
      <pre id="li2024taptr_bibtex" class="pre pre-scrollable collapse">@article{li2024taptr,
        title={TAPTR: Tracking Any Point with Transformers as Detection},
        author={Li, Hongyang and Zhang, Hao and Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Lei},
        journal={arXiv preprint arXiv:2403.13042},
        year={2024}
      }
</pre>
    
    </span>
  </div>
</div>
</li>

<!-- Second -->
<li>
  <div class="pub row">
<div class="pub-image col-md-3">
  
  <img src="/assets/pdf/zhang2023llava.png" >
  
</div>
<div class="col-md-9">
  <span id="zhang2023llava"><div class="csl-block csl-content"><div class="csl-block csl-title"><b>LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models</b></div><div class="csl-block csl-author"> Hao Zhang*, <b>Hongyang Li*</b>, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Lei Zhang, Chunyuan Li, Jianwei Yang</div><div class="csl-block csl-event"><i>ECCV</i>, 2024</div></div></span>

  <span id="zhang2023llava_materials">
    <ul class="tag">
      
  
      <span><a class="bib-materials" data-target="#zhang2023llava_bibtex" data-toggle="collapse" href="#zhang2023llava" onclick="return false" aria-expanded="false">BibTex</a></span>
  
  
      
        
        <span>
          <a class="bib-materials" href="https://arxiv.org/abs/2312.02949">arXiv</a>
          (<a class="bib-materials" href="https://arxiv.org/pdf/2312.02949">pdf</a>)
        </span>
        <span>
          <a class="project-page" href="https://llava-vl.github.io/llava-grounding/">Project Page</a>
        </span>
        <span>
          <a class="github" href="https://github.com/UX-Decoder/LLaVA-Grounding">Github</a>
        </span>
      


      

    </ul>
  
    
  
    <pre id="zhang2023llava_bibtex" class="pre pre-scrollable collapse">@article{zhang2023llava,
      title={Llava-grounding: Grounded visual chat with large multimodal models},
      author={Zhang, Hao and Li, Hongyang and Li, Feng and Ren, Tianhe and Zou, Xueyan and Liu, Shilong and Huang, Shijia and Gao, Jianfeng and Zhang, Lei and Li, Chunyuan and others},
      journal={arXiv preprint arXiv:2312.02949},
      year={2023}
    }
</pre>
  
  </span>
</div>
</div>
</li>

<!-- Third -->
<li>
  <div class="pub row">
<div class="pub-image col-md-3">
  
  <img src="/assets/pdf/li2023dfa3d.png" >
  
</div>
<div class="col-md-9">
  <span id="li2023dfa3d"><div class="csl-block csl-content"><div class="csl-block csl-title"><b>DFA3D: 3D Deformable Attention For 2D-to-3D Feature Lifting</b></div><div class="csl-block csl-author"> <b>Hongyang Li*</b>, Hao Zhang*, Zhaoyang Zeng, Shilong Liu, Feng Li, Tianhe Ren, Lei Zhang</div><div class="csl-block csl-event"><i>ICCV</i>, 2023</div></div></span>

  <span id="li2023dfa3d_materials">
    <ul class="tag">
      
  
      <span><a class="bib-materials" data-target="#li2023dfa3d_bibtex" data-toggle="collapse" href="#li2023dfa3d" onclick="return false" aria-expanded="false">BibTex</a></span>
  
  
      
        
        <span>
          <a class="bib-materials" href="https://arxiv.org/abs/2307.12972">arXiv</a>
          (<a class="bib-materials" href="https://arxiv.org/pdf/2307.12972">pdf</a>)
        </span>
        <!-- <span>
          <a class="project-page" href="https://llava-vl.github.io/llava-grounding/">Project Page</a>
        </span> -->
        <span>
          <a class="github" href="https://github.com/IDEA-Research/3D-deformable-attention">Github</a>
        </span>
      


      

    </ul>
  
    
  
    <pre id="li2023dfa3d_bibtex" class="pre pre-scrollable collapse">@inproceedings{li2023dfa3d,
      title={DFA3D: 3D Deformable Attention For 2D-to-3D Feature Lifting},
      author={Li, Hongyang and Zhang, Hao and Zeng, Zhaoyang and Liu, Shilong and Li, Feng and Ren, Tianhe and Zhang, Lei},
      booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
      pages={6684--6693},
      year={2023}
    }
</pre>
  
  </span>
</div>
</div>
</li>
 
<!-- Forth -->
<li>
  <div class="pub row">
<div class="pub-image col-md-3">
  
  <img src="/assets/pdf/zhang2023bev.png" >
  
</div>
<div class="col-md-9">
  <span id="zhang2023bev"><div class="csl-block csl-content"><div class="csl-block csl-title"><b>DA-BEV: Depth Aware BEV Transformer for 3D Object Detection</b></div><div class="csl-block csl-author"> Hao Zhang*, <b>Hongyang Li*</b>, Zhaoyang Zeng, Shilong Liu, Feng Li, Tianhe Ren, Lei Zhang</div><div class="csl-block csl-event"><i>arxiv</i>, 2023</div></div></span>

  <span id="zhang2023bev_materials">
    <ul class="tag">
      
  
      <span><a class="bib-materials" data-target="#zhang2023bev_bibtex" data-toggle="collapse" href="#zhang2023bev" onclick="return false" aria-expanded="false">BibTex</a></span>
  
  
      
        
        <span>
          <a class="bib-materials" href="https://arxiv.org/abs/2302.13002">arXiv</a>
          (<a class="bib-materials" href="https://arxiv.org/pdf/2302.13002">pdf</a>)
        </span>
        <!-- <span>
          <a class="project-page" href="https://llava-vl.github.io/llava-grounding/">Project Page</a>
        </span> -->
        <!-- <span>
          <a class="github" href="https://github.com/IDEA-Research/3D-deformable-attention">Github</a>
        </span> -->
      


      

    </ul>
  
    
  
    <pre id="zhang2023bev_bibtex" class="pre pre-scrollable collapse">@article{zhang2023bev,
      title={Da-bev: Depth aware bev transformer for 3d object detection},
      author={Zhang, Hao and Li, Hongyang and Liao, Xingyu and Li, Feng and Liu, Shilong and Ni, Lionel M and Zhang, Lei},
      journal={arXiv e-prints},
      pages={arXiv--2302},
      year={2023}
    }
</pre>
  
  </span>
</div>
</div>
</li>


<!-- Fifth -->
<li>
  <div class="pub row">
<div class="pub-image col-md-3">
  
  <img src="/assets/pdf/li2022dcl.png" >
  
</div>
<div class="col-md-9">
  <span id="li2022dcl"><div class="csl-block csl-content"><div class="csl-block csl-title"><b>DCL-Net: Deep Correspondence Learning Network for 6D Pose Estimation</b></div><div class="csl-block csl-author"> <b>Hongyang Li*</b>, Jiehong Lin*, Kui Jia</div><div class="csl-block csl-event"><i>ECCV</i>, 2022</div></div></span>

  <span id="li2022dcl_materials">
    <ul class="tag">
      
  
      <span><a class="bib-materials" data-target="#li2022dcl_bibtex" data-toggle="collapse" href="#li2022dcl" onclick="return false" aria-expanded="false">BibTex</a></span>
  
  
      
        
        <span>
          <a class="bib-materials" href="https://arxiv.org/abs/2210.05232">arXiv</a>
          (<a class="bib-materials" href="https://arxiv.org/pdf/2210.05232">pdf</a>)
        </span>
        <!-- <span>
          <a class="project-page" href="https://llava-vl.github.io/llava-grounding/">Project Page</a>
        </span> -->
        <span>
          <a class="github" href="https://github.com/Gorilla-Lab-SCUT/DCL-Net">Github</a>
        </span>
      


      

    </ul>
  
    
  
    <pre id="li2022dcl_bibtex" class="pre pre-scrollable collapse">@inproceedings{li2022dcl,
      title={DCL-Net: Deep Correspondence Learning Network for 6D Pose Estimation},
      author={Li, Hongyang and Lin, Jiehong and Jia, Kui},
      booktitle={European Conference on Computer Vision},
      pages={369--385},
      year={2022},
      organization={Springer}
    }
</pre>
  
  </span>
</div>
</div>
</li>


<!-- Sixth -->
<li>
  <div class="pub row">
<div class="pub-image col-md-3">
  
  <img src="/assets/pdf/lin2021sparse.png" >
  
</div>
<div class="col-md-9">
  <span id="lin2021sparse"><div class="csl-block csl-content"><div class="csl-block csl-title"><b>Sparse Steerable Convolutions: An Efficient Learning of SE(3)-Equivariant Features for Estimation and Tracking of Object Poses in 3D Space</b></div><div class="csl-block csl-author"> Jiehong Lin*, <b>Hongyang Li*</b>, Kui Jia</div><div class="csl-block csl-event"><i>NeurIPS</i>, 2021</div></div></span>

  <span id="lin2021sparse_materials">
    <ul class="tag">
      
  
      <span><a class="bib-materials" data-target="#lin2021sparse_bibtex" data-toggle="collapse" href="#lin2021sparse" onclick="return false" aria-expanded="false">BibTex</a></span>
  
  
      
        
        <span>
          <a class="bib-materials" href="https://arxiv.org/abs/2111.07383">arXiv</a>
          (<a class="bib-materials" href="https://arxiv.org/pdf/2111.07383">pdf</a>)
        </span>
        <!-- <span>
          <a class="project-page" href="https://llava-vl.github.io/llava-grounding/">Project Page</a>
        </span> -->
        <span>
          <a class="github" href="https://github.com/Gorilla-Lab-SCUT/SS-Conv">Github</a>
        </span>
      


      

    </ul>
  
    
  
    <pre id="lin2021sparse_bibtex" class="pre pre-scrollable collapse">@article{lin2021sparse,
      title={Sparse steerable convolutions: An efficient learning of se (3)-equivariant features for estimation and tracking of object poses in 3d space},
      author={Lin, Jiehong and Li, Hongyang and Chen, Ke and Lu, Jiangbo and Jia, Kui},
      journal={Advances in Neural Information Processing Systems},
      volume={34},
      pages={16779--16790},
      year={2021}
    }
</pre>
  
  </span>
</div>
</div>
</li>


</ol>


<hr>
<h1>
  Awards
</h1>

<ul>

<li>
  <p>Principal's Scholarship & National Scholarships in 2023.</p>
</li>
<li>
  <p>Principal's Scholarship & National Scholarships in 2022.</p>
</li>
<li>
  <p>The First Prize Scholarship * 2 & The Second Prize Scholarship during 2018-2022.</p>
</li>
</ul>

</div>

      </div>
      <div class="footer row">
        <p>
    <div class="copyright">
        &copy; <a href="http://TODO.org">hongyangli</a> 2024,
        built using 
        <a href="http://jekyllrb.com">jekyll</a>, 
        <a href="https://github.com/inukshuk/jekyll-scholar">jekyll-scholar</a> and 
        <a href="http://getbootstrap.com">bootstrap</a>. 
    </div>
    <div class="counter">
      <!-- TODO: Update this link. -->
      <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=uVOnPzbnu4G1ic2vusvZTtAdpfenj-5hpUr1Ym3eMAY&cl=ffffff&w=a"></script>
    </div>
</p>        
      </div>
    </div>
    <script src="https://code.jquery.com/jquery.js"></script>
    <script src="/assets/js/bootstrap.min.js"></script>
  </body>
</html>


